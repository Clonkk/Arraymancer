<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Arraymancer - optimizers</title>

  <link href="docutils.css" rel="stylesheet" type="text/css"/>
  <link href="nav.css" rel="stylesheet" type="text/css"/>

  <link href='http://fonts.googleapis.com/css?family=Raleway:400,600,900' rel='stylesheet' type='text/css'/>
  <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>
</head>
<body>
<a href="https://github.com/mratsim/arraymancer"><img style="position: fixed; top: 0; right: 0; border: 0; z-index: 10;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<header>
  <a class="pagetitle" href="index.html">Arraymancer</a>
  <span>
    <a href="#">Technical reference</a>
    <ul class="monospace">
      <span>
        <a href="#">Core tensor API</a>
        <ul class="monospace">
          <li><a href="tensor.accessors_macros_read.html">tensor.accessors_macros_read</a></li>
          <li><a href="tensor.accessors_macros_syntax.html">tensor.accessors_macros_syntax</a></li>
          <li><a href="tensor.accessors_macros_write.html">tensor.accessors_macros_write</a></li>
          <li><a href="tensor.accessors.html">tensor.accessors</a></li>
          <li><a href="tensor.aggregate.html">tensor.aggregate</a></li>
          <li><a href="tensor.comparison.html">tensor.comparison</a></li>
          <li><a href="tensor.data_structure.html">tensor.data_structure</a></li>
          <li><a href="tensor.display.html">tensor.display</a></li>
          <li><a href="tensor.display_cuda.html">tensor.display_cuda</a></li>
          <li><a href="tensor.exporting.html">tensor.exporting</a></li>
          <li><a href="tensor.filling_data.html">tensor.filling_data</a></li>
          <li><a href="tensor.higher_order_applymap.html">tensor.higher_order_applymap</a></li>
          <li><a href="tensor.higher_order_foldreduce.html">tensor.higher_order_foldreduce</a></li>
          <li><a href="tensor.init_cpu.html">tensor.init_cpu</a></li>
          <li><a href="tensor.init_cuda.html">tensor.init_cuda</a></li>
          <li><a href="tensor.init_opencl.html">tensor.init_opencl</a></li>
          <li><a href="tensor.init_copy_cpu.html">tensor.init_copy_cpu</a></li>
          <li><a href="tensor.init_copy_cuda.html">tensor.init_copy_cuda</a></li>
          <li><a href="tensor.lapack.html">tensor.lapack</a></li>
          <li><a href="tensor.math_functions.html">tensor.math_functions</a></li>
          <li><a href="tensor.operators_blas_l1.html">tensor.operators_blas_l1</a></li>
          <li><a href="tensor.operators_blas_l1_cuda.html">tensor.operators_blas_l1_cuda</a></li>
          <li><a href="tensor.operators_blas_l1_opencl.html">tensor.operators_blas_l1_opencl</a></li>
          <li><a href="tensor.operators_blas_l2l3.html">tensor.operators_blas_l2l3</a></li>
          <li><a href="tensor.operators_blas_l2l3_cuda.html">tensor.operators_blas_l2l3_cuda</a></li>
          <li><a href="tensor.operators_blas_l2l3_opencl.html">tensor.operators_blas_l2l3_opencl</a></li>
          <li><a href="tensor.operators_broadcasted.html">tensor.operators_broadcasted</a></li>
          <li><a href="tensor.operators_broadcasted_cuda.html">tensor.operators_broadcasted_cuda</a></li>
          <li><a href="tensor.operators_broadcasted_opencl.html">tensor.operators_broadcasted_opencl</a></li>
          <li><a href="tensor.operators_comparison.html">tensor.operators_comparison</a></li>
          <li><a href="tensor.operators_logical.html">tensor.operators_logical</a></li>
          <li><a href="tensor.optim_ops_fusion.html">tensor.optim_ops_fusion</a></li>
          <li><a href="tensor.shapeshifting.html">tensor.shapeshifting</a></li>
          <li><a href="tensor.shapeshifting_cuda.html">tensor.shapeshifting_cuda</a></li>
          <li><a href="tensor.shapeshifting_opencl.html">tensor.shapeshifting_opencl</a></li>
          <li><a href="tensor.syntactic_sugar.html">tensor.syntactic_sugar</a></li>
          <li><a href="tensor.ufunc.html">tensor.ufunc</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neural network API</a>
        <ul class="monospace">
          <li><a href="nn_dsl.dsl_core.html">Neural network: Declaration</a></li>
          <li><a href="nn_activation.relu.html">Activation: Relu (Rectified linear Unit)</a></li>
          <li><a href="nn_activation.sigmoid.html">Activation: Sigmoid</a></li>
          <li><a href="nn_activation.tanh.html">Activation: Tanh</a></li>
          <li><a href="nn_layers.conv2D.html">Layers: Convolution 2D</a></li>
          <li><a href="nn_layers.embedding.html">Layers: Embedding</a></li>
          <li><a href="nn_layers.gru.html">Layers: GRU (Gated Linear Unit)</a></li>
          <li><a href="nn_layers.linear.html">Layers: Linear/Dense</a></li>
          <li><a href="nn_layers.maxpool2D.html">Layers: Maxpool 2D</a></li>
          <li><a href="nn_loss.cross_entropy_losses.html">Loss: Cross-Entropy losses</a></li>
          <li><a href="nn_loss.mean_square_error_loss.html">Loss: Mean Square Error</a></li>
          <li><a href="nn_optimizers.optimizers.html">Optimizers</a></li>
          <li><a href="nn_shapeshifting.reshape_flatten.html">Reshape & Flatten</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Linear algebra, stats, ML</a>
        <ul class="monospace">
          <li><a href="la.decomposition.html">Eigenvalue decomposition</a></li>
          <li><a href="la.decomposition_rand.html">Randomized Truncated SVD</a></li>
          <li><a href="la.least_squares.html">Least squares solver</a></li>
          <li><a href="la.linear_systems.html">Linear systems solver</a></li>
          <li><a href="la.special_matrices.html">Special linear algebra matrices</a></li>
          <li><a href="stats.stats.html">Statistics</a></li>
          <li><a href="ml.pca.html">Principal Component Analysis (PCA)</a></li>
          <li><a href="ml.accuracy_score.html">Accuracy score</a></li>
          <li><a href="ml.common_error_functions.html">Common errors, MAE and MSE (L1, L2 loss)</a></li>
          <li><a href="ml.kmeans.html">K-Means</a></li>
        </ul>
      </span>
      <span>
        <a href="#">IO & Datasets</a>
        <ul class="monospace">
          <li><a href="datasets.mnist.html">MNIST</a></li>
          <li><a href="datasets.imdb.html">IMDB</a></li>
          <li><a href="io.io_csv.html">CSV reading and writing</a></li>
          <li><a href="io.io_hdf5.html">HDF5 files reading and writing</a></li>
          <li><a href="io.io_image.html">Images reading and writing</a></li>
          <li><a href="io.io_npy.html">Numpy files reading and writing</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Autograd</a>
        <ul class="monospace">
          <li><a href="ag.autograd_common.html">Data structure</a></li>
          <li><a href="ag.gates_basic.html">Basic operations</a></li>
          <li><a href="ag.gates_blas.html">Linear algebra operations</a></li>
          <li><a href="ag.gates_hadamard.html">Hadamard product (elementwise matrix multiply)</a></li>
          <li><a href="ag.gates_reduce.html">Reduction operations</a></li>
          <li><a href="ag.gates_shapeshifting_concat_split.html">Concatenation, stacking, splitting, chunking operations</a></li>
          <li><a href="ag.gates_shapeshifting_views.html">Linear algebra operations</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neuralnet primitives</a>
        <ul class="monospace">
          <li><a href="nnp.nnp_activation.html">Activations</a></li>
          <li><a href="nnp.nnp_convolution.html">Convolution 2D</a></li>
          <li><a href="nnp.nnp_conv2D_cudnn.html">Convolution 2D - CuDNN</a></li>
          <li><a href="nnp.nnp_embedding.html">Linear / Dense layer</a></li>
          <li><a href="nnp.nnp_gru.html">Linear / Dense layer</a></li>
          <li><a href="nnp.nnp_linear.html">Linear / Dense layer</a></li>
          <li><a href="nnp.nnp_maxpooling.html">Maxpooling</a></li>
          <li><a href="nnp.nnp_numerical_gradient.html">Numerical gradient</a></li>
          <li><a href="nnp.nnp_sigmoid_cross_entropy.html">Sigmoid Cross-Entropy loss</a></li>
          <li><a href="nnp.nnp_softmax_cross_entropy.html">Softmax Cross-Entropy loss</a></li>
          <li><a href="nnp.nnp_softmax.html">Softmax</a></li>
        </ul>
      </span>
    </ul>
  </span>
  <span>
    <a href="#">Tutorial</a>
    <ul class="monospace">
      <li><a href="tuto.first_steps.html">First steps</a></li>
      <li><a href="tuto.slicing.html">Taking a slice of a tensor</a></li>
      <li><a href="tuto.linear_algebra.html">Matrix & vectors operations</a></li>
      <li><a href="tuto.broadcasting.html">Broadcasted operations</a></li>
      <li><a href="tuto.shapeshifting.html">Transposing, Reshaping, Permuting, Concatenating</a></li>
      <li><a href="tuto.map_reduce.html">Map & Reduce</a></li>
      <li><a href="tuto.iterators.html">Basic iterators</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Spellbook (How-To&apos;s)</a>
    <ul class="monospace">
      <li><a href="howto.type_conversion.html">How to convert a Tensor type?</a></li>
      <li><a href="howto.ufunc.html">How to create a new universal function?</a></li>
      <li><a href="howto.perceptron.html">How to create a multilayer perceptron?</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Under the hood</a>
    <ul class="monospace">
      <li><a href="uth.speed.html">How Arraymancer achieves its speed?</a></li>
      <li><a href="uth.copy_semantics.html">Why does `=` share data by default aka reference semantics?</a></li>
      <li><a href="uth.opencl_cuda_nim.html">Working with OpenCL and Cuda in Nim</a></li>
    </ul>
  </span>
</header>
<article id="documentId">
  <div class="container">
    <h1 class="title">optimizers</h1>
    <div class="row">
  <div class="three columns">
  <div id="global-links">
    <ul class="simple">
    </ul>
  </div>
  <div id="searchInputDiv">
    Search: <input type="text" id="searchInput"
      onkeyup="search()" />
  </div>
  <div>
    Group by:
    <select onchange="groupBy(this.value)">
      <option value="section">Section</option>
      <option value="type">Type</option>
    </select>
  </div>
  <ul class="simple simple-toc" id="toc-list">
<li>
  <a class="reference reference-toplevel" href="#6" id="56">Imports</a>
  <ul class="simple simple-toc-section">
    
  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#7" id="57">Types</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#Sgd"
    title="Sgd[TT] = object
  params*: seq[Variable[TT]]
  lr*: TT.T"><wbr />Sgd<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#SgdMomentum"
    title="SgdMomentum[TT] = object
  params*: seq[Variable[TT]]
  lr*: TT.T                    ## Learning rate
  momentum*: TT.T              ## Value of the momentum
  moments: seq[TT]             ## Moments for momentum
  decay: TT.T                  ## Learning rate decay
  nesterov: bool               ## Flag for Nesterov momentum"><wbr />Sgd<wbr />Momentum<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#Adam"
    title="Adam[TT] = object
  params: seq[Variable[TT]]    ## Learnable weights
  learning_rate: TT.T
  beta1, beta2: TT.T            ## Decays on first and second moment
  beta1_t, beta2_t: TT.T        ## Current decay
  first_moments: seq[TT]       ## Exponential moving averages (mean estimation)
  second_moments: seq[TT]      ## Exponential moving averages squared (uncentered variance)
  epsilon: TT.T                ## Epsilon for numerical stability when dividing"><wbr />Adam<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#Optimizer"
    title="Optimizer[TT] = Sgd[TT] or Adam[TT]"><wbr />Optimizer<span class="attachedType"></span></a></li>

  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#12" id="62">Procs</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#newSGD%2Cvarargs%5BVariable%5BTensor%5BT%5D%5D%5D%2CT"
    title="newSGD[T](params: varargs[Variable[Tensor[T]]]; learning_rate: T): Sgd[Tensor[T]]"><wbr />new<wbr />SGD<span class="attachedType">Sgd</span></a></li>
  <li><a class="reference" href="#update%2CSgd"
    title="update(self: Sgd)"><wbr />update<span class="attachedType">Sgd</span></a></li>
  <li><a class="reference" href="#update%2CSgdMomentum"
    title="update(self: var SgdMomentum)"><wbr />update<span class="attachedType">SgdMomentum</span></a></li>
  <li><a class="reference" href="#update%2CAdam"
    title="update(self: var Adam)"><wbr />update<span class="attachedType">Adam</span></a></li>
  <li><a class="reference" href="#zeroGrads%2COptimizer"
    title="zeroGrads(o: Optimizer)"><wbr />zero<wbr />Grads<span class="attachedType">Optimizer</span></a></li>

  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#13" id="63">Funcs</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#optimizerSGD%2CM%2CT"
    title="optimizerSGD[M, T](model: M; learning_rate: T): Sgd[Tensor[T]]"><wbr />optimizer<wbr />SGD<span class="attachedType">Sgd</span></a></li>
  <li><a class="reference" href="#optimizerSGDMomentum%2CM%2CT%2Ctype%28T%280.0%29%29%2Ctype%28T%280.0%29%29"
    title="optimizerSGDMomentum[M, T](model: M; learning_rate: T; momentum = T(0.0);
                          decay = T(0.0); nesterov = false): SgdMomentum[Tensor[T]]"><wbr />optimizer<wbr />SGDMomentum<span class="attachedType">SgdMomentum</span></a></li>
  <li><a class="reference" href="#optimizerAdam%2CM%2CT%2Ctype%28T%280.9%29%29%2Ctype%28T%280.999%29%29%2Ctype%28T%281e-08%29%29"
    title="optimizerAdam[M, T](model: M; learning_rate: T = T(0.001); beta1 = T(0.9);
                   beta2 = T(0.999); eps = T(1e-08)): Adam[Tensor[T]]"><wbr />optimizer<wbr />Adam<span class="attachedType">Adam</span></a></li>

  </ul>
</li>

</ul>

  </div>
  <div class="nine columns" id="content">
  <div id="tocRoot"></div>
  
  <p class="module-desc"></p>
  <div class="section" id="6">
<h1><a class="toc-backref" href="#6">Imports</a></h1>
<dl class="item">
<a class="reference external" href="tensor.html">tensor</a>, <a class="reference external" href="higher_order_applymap.html">higher_order_applymap</a>, <a class="reference external" href="autograd.html">autograd</a>, <a class="reference external" href="ast_utils.html">ast_utils</a>
</dl></div>
<div class="section" id="7">
<h1><a class="toc-backref" href="#7">Types</a></h1>
<dl class="item">
<a id="Sgd"></a>
<dt><pre><a href="optimizers.html#Sgd"><span class="Identifier">Sgd</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">object</span>
  <span class="Identifier">params</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><a href="autograd_common.html#Variable"><span class="Identifier">Variable</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span><span class="Other">]</span>
  <span class="Identifier">lr</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>
</pre></dt>
<dd>

Stochastic gradient descent without momentum.
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L27"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L27" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="SgdMomentum"></a>
<dt><pre><a href="optimizers.html#SgdMomentum"><span class="Identifier">SgdMomentum</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">object</span>
  <span class="Identifier">params</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><a href="autograd_common.html#Variable"><span class="Identifier">Variable</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span><span class="Other">]</span>
  <span class="Identifier">lr</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>                    <span class="Comment">## Learning rate</span>
  <span class="Identifier">momentum</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>              <span class="Comment">## Value of the momentum</span>
  <span class="Identifier">moments</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span>             <span class="Comment">## Moments for momentum</span>
  <span class="Identifier">decay</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>                  <span class="Comment">## Learning rate decay</span>
  <span class="Identifier">nesterov</span><span class="Other">:</span> <span class="Identifier">bool</span>               <span class="Comment">## Flag for Nesterov momentum</span>
  </pre></dt>
<dd>

Stochastic gradient descent with momentum. Details on Nesterov momentum can be found in <a class="reference external" href="http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf">Sutskever et. al. 2013</a>
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L89"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L89" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="Adam"></a>
<dt><pre><a href="optimizers.html#Adam"><span class="Identifier">Adam</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">object</span>
  <span class="Identifier">params</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><a href="autograd_common.html#Variable"><span class="Identifier">Variable</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span><span class="Other">]</span>    <span class="Comment">## Learnable weights</span>
  <span class="Identifier">learning_rate</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>
  <span class="Identifier">beta1</span><span class="Other">,</span> <span class="Identifier">beta2</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>            <span class="Comment">## Decays on first and second moment</span>
  <span class="Identifier">beta1_t</span><span class="Other">,</span> <span class="Identifier">beta2_t</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>        <span class="Comment">## Current decay</span>
  <span class="Identifier">first_moments</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span>       <span class="Comment">## Exponential moving averages (mean estimation)</span>
  <span class="Identifier">second_moments</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span>      <span class="Comment">## Exponential moving averages squared (uncentered variance)</span>
  <span class="Identifier">epsilon</span><span class="Other">:</span> <span class="Identifier">TT</span><span class="Other">.</span><span class="Identifier">T</span>                <span class="Comment">## Epsilon for numerical stability when dividing</span>
  </pre></dt>
<dd>

Adaptative Moment Estimation
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L191"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L191" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="Optimizer"></a>
<dt><pre><a href="optimizers.html#Optimizer"><span class="Identifier">Optimizer</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span> <span class="Other">=</span> <a href="optimizers.html#Sgd"><span class="Identifier">Sgd</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span> <span class="Keyword">or</span> <a href="optimizers.html#Adam"><span class="Identifier">Adam</span></a><span class="Other">[</span><span class="Identifier">TT</span><span class="Other">]</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L265"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L265" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>
<div class="section" id="12">
<h1><a class="toc-backref" href="#12">Procs</a></h1>
<dl class="item">
<a id="newSGD,varargs[Variable[Tensor[T]]],T"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#newSGD%2Cvarargs%5BVariable%5BTensor%5BT%5D%5D%5D%2CT"><span class="Identifier">newSGD</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">params</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><a href="autograd_common.html#Variable"><span class="Identifier">Variable</span></a><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">learning_rate</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">)</span><span class="Other">:</span> <a href="optimizers.html#Sgd"><span class="Identifier">Sgd</span></a><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma">
    <span class="Identifier">deprecated</span><span class="Other">:</span> <span class="StringLit">&quot;Use the optimizer macro instead&quot;</span></span><span class="Other">.}</span></span></pre></dt>
<dd>
  <div class="deprecation-message">
    <b>Deprecated:</b> Use the optimizer macro instead
  </div>


&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L32"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L32" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="update,Sgd"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#update%2CSgd"><span class="Identifier">update</span></a><span class="Other">(</span><span class="Identifier">self</span><span class="Other">:</span> <a href="optimizers.html#Sgd"><span class="Identifier">Sgd</span></a><span class="Other">)</span></pre></dt>
<dd>

<p>Performs an optimization update.</p>
<p>Parameters:</p>
<ul class="simple"><li><tt class="docutils literal"><span class="pre">self</span></tt> A SGD optimizer to update.</li>
</ul>
<p>This proc will update the weights in the model associated with the input</p>
<dl class="docutils"><dt>optimizer according to the following rule:</dt>
<dd><tt class="docutils literal"><span class="pre">w = w - lr * gradient</span></tt></dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L35"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L35" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="update,SgdMomentum"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#update%2CSgdMomentum"><span class="Identifier">update</span></a><span class="Other">(</span><span class="Identifier">self</span><span class="Other">:</span> <span class="Keyword">var</span> <a href="optimizers.html#SgdMomentum"><span class="Identifier">SgdMomentum</span></a><span class="Other">)</span></pre></dt>
<dd>

<p>Performs an optimization update.</p>
<p>Parameters:</p>
<ul class="simple"><li><tt class="docutils literal"><span class="pre">self</span></tt> A SGDMomentum optimizer to update.</li>
</ul>
<p>This proc will update the weights in the model associated with the input</p>
<dl class="docutils"><dt>optimizer according to the following rule:</dt>
<dd><tt class="docutils literal"><span class="pre">w = w - lr * gradient + m * moment</span></tt></dd>
<dt>If nesterov is set to true then the following rule is applied instead:</dt>
<dd><p><tt class="docutils literal"><span class="pre">w = w - lr * gradient + m * v</span></tt></p>
<p><tt class="docutils literal"><span class="pre">v = - lr * gradient + m * moment</span></tt></p>
</dd>
</dl>
<p>Where in both cases the <tt class="docutils literal"><span class="pre">moment</span></tt> is the gradient change applied in the previous update step and <tt class="docutils literal"><span class="pre">m</span></tt> is the momentum.</p>
<p>If <tt class="docutils literal"><span class="pre">decay</span></tt> is greater than 0, the learning rate will be modified each</p>
<dl class="docutils"><dt>call according to the following:</dt>
<dd><tt class="docutils literal"><span class="pre">lr = lr * 1/(1 + decay)</span></tt></dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L100"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L100" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="update,Adam"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#update%2CAdam"><span class="Identifier">update</span></a><span class="Other">(</span><span class="Identifier">self</span><span class="Other">:</span> <span class="Keyword">var</span> <a href="optimizers.html#Adam"><span class="Identifier">Adam</span></a><span class="Other">)</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L201"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L201" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="zeroGrads,Optimizer"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#zeroGrads%2COptimizer"><span class="Identifier">zeroGrads</span></a><span class="Other">(</span><span class="Identifier">o</span><span class="Other">:</span> <a href="optimizers.html#Optimizer"><span class="Identifier">Optimizer</span></a><span class="Other">)</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L267"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L267" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>
<div class="section" id="13">
<h1><a class="toc-backref" href="#13">Funcs</a></h1>
<dl class="item">
<a id="optimizerSGD,M,T"></a>
<dt><pre><span class="Keyword">func</span> <a href="#optimizerSGD%2CM%2CT"><span class="Identifier">optimizerSGD</span></a><span class="Other">[</span><span class="Identifier">M</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">model</span><span class="Other">:</span> <span class="Identifier">M</span><span class="Other">;</span> <span class="Identifier">learning_rate</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">)</span><span class="Other">:</span> <a href="optimizers.html#Sgd"><span class="Identifier">Sgd</span></a><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span></pre></dt>
<dd>

<p>Create a SGD optimizer that will update the model weight</p>
<p>Parameters:</p>
<ul class="simple"><li><tt class="docutils literal"><span class="pre">model</span></tt> Model to optimize.</li>
<li><tt class="docutils literal"><span class="pre">learning_rate</span></tt> Learning rate.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>A SGD optimizer with the given learning rate.</li>
</ul>
<dl class="docutils"><dt>Future TODO:</dt>
<dd>Rename to <tt class="docutils literal"><span class="pre">optimize[M](model: M, OptimizerKind: typedesc[SGD], learning_rate: SomeFloat):</span></tt></dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L55"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L55" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="optimizerSGDMomentum,M,T,type(T(0.0)),type(T(0.0))"></a>
<dt><pre><span class="Keyword">func</span> <a href="#optimizerSGDMomentum%2CM%2CT%2Ctype%28T%280.0%29%29%2Ctype%28T%280.0%29%29"><span class="Identifier">optimizerSGDMomentum</span></a><span class="Other">[</span><span class="Identifier">M</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">model</span><span class="Other">:</span> <span class="Identifier">M</span><span class="Other">;</span> <span class="Identifier">learning_rate</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">momentum</span> <span class="Other">=</span> <span class="Identifier">T</span><span class="Other">(</span><span class="FloatNumber">0.0</span><span class="Other">)</span><span class="Other">;</span>
                              <span class="Identifier">decay</span> <span class="Other">=</span> <span class="Identifier">T</span><span class="Other">(</span><span class="FloatNumber">0.0</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">nesterov</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <a href="optimizers.html#SgdMomentum"><span class="Identifier">SgdMomentum</span></a><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span></pre></dt>
<dd>

<p>Create a SGD optimizer with optional momentum that will update the model weight</p>
<p>Parameters:</p>
<ul class="simple"><li><tt class="docutils literal"><span class="pre">model</span></tt> Model to optimize.</li>
<li><tt class="docutils literal"><span class="pre">learning_rate</span></tt> Learning rate.</li>
<li><tt class="docutils literal"><span class="pre">momentum</span></tt> Momentum.</li>
<li><tt class="docutils literal"><span class="pre">decay</span></tt> How much the learning rate will decay each update.</li>
<li><tt class="docutils literal"><span class="pre">nesterov</span></tt> Whether to use Nesterov momentum or not.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>A SGD optimizer with momentum with the given parameters.</li>
</ul>
<dl class="docutils"><dt>Future TODO:</dt>
<dd>Rename to <tt class="docutils literal"><span class="pre">optimize[M](model: M, OptimizerKind: typedesc[SGDMomentum], learning_rate: SomeFloat):</span></tt></dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L148"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L148" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="optimizerAdam,M,T,type(T(0.9)),type(T(0.999)),type(T(1e-08))"></a>
<dt><pre><span class="Keyword">func</span> <a href="#optimizerAdam%2CM%2CT%2Ctype%28T%280.9%29%29%2Ctype%28T%280.999%29%29%2Ctype%28T%281e-08%29%29"><span class="Identifier">optimizerAdam</span></a><span class="Other">[</span><span class="Identifier">M</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">model</span><span class="Other">:</span> <span class="Identifier">M</span><span class="Other">;</span> <span class="Identifier">learning_rate</span><span class="Other">:</span> <span class="Identifier">T</span> <span class="Other">=</span> <span class="Identifier">T</span><span class="Other">(</span><span class="FloatNumber">0.001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">beta1</span> <span class="Other">=</span> <span class="Identifier">T</span><span class="Other">(</span><span class="FloatNumber">0.9</span><span class="Other">)</span><span class="Other">;</span>
                       <span class="Identifier">beta2</span> <span class="Other">=</span> <span class="Identifier">T</span><span class="Other">(</span><span class="FloatNumber">0.999</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">eps</span> <span class="Other">=</span> <span class="Identifier">T</span><span class="Other">(</span><span class="FloatNumber">1e-08</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <a href="optimizers.html#Adam"><span class="Identifier">Adam</span></a><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span></pre></dt>
<dd>

Create a Adam optimizer that will update the model weight
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/version-1-0/src/nn/optimizers/optimizers.nim#L227"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn/optimizers/optimizers.nim#L227" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>

  </div>
</div>

    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small>Made with Nim. Generated: 2020-01-08 23:22:38 UTC</small>
      </div>
    </div>
  </div>
</article>
</body>
</html>
