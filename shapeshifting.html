<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--  This file is generated by Nim. -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Favicon -->
<link rel="shortcut icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAUAAAAF////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAIAAABbAAAAlQAAAKIAAACbAAAAmwAAAKIAAACVAAAAWwAAAAL///8A////AP///wD///8A////AAAAABQAAADAAAAAYwAAAA3///8A////AP///wD///8AAAAADQAAAGMAAADAAAAAFP///wD///8A////AP///wAAAACdAAAAOv///wD///8A////AP///wD///8A////AP///wD///8AAAAAOgAAAJ3///8A////AP///wAAAAAnAAAAcP///wAAAAAoAAAASv///wD///8A////AP///wAAAABKAAAAKP///wAAAABwAAAAJ////wD///8AAAAAgQAAABwAAACIAAAAkAAAAJMAAACtAAAAFQAAABUAAACtAAAAkwAAAJAAAACIAAAAHAAAAIH///8A////AAAAAKQAAACrAAAAaP///wD///8AAAAARQAAANIAAADSAAAARf///wD///8AAAAAaAAAAKsAAACk////AAAAADMAAACcAAAAnQAAABj///8A////AP///wAAAAAYAAAAGP///wD///8A////AAAAABgAAACdAAAAnAAAADMAAAB1AAAAwwAAAP8AAADpAAAAsQAAAE4AAAAb////AP///wAAAAAbAAAATgAAALEAAADpAAAA/wAAAMMAAAB1AAAAtwAAAOkAAAD/AAAA/wAAAP8AAADvAAAA3gAAAN4AAADeAAAA3gAAAO8AAAD/AAAA/wAAAP8AAADpAAAAtwAAAGUAAAA/AAAA3wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAADfAAAAPwAAAGX///8A////AAAAAEgAAADtAAAAvwAAAL0AAADGAAAA7wAAAO8AAADGAAAAvQAAAL8AAADtAAAASP///wD///8A////AP///wD///8AAAAAO////wD///8A////AAAAAIcAAACH////AP///wD///8AAAAAO////wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A//8AAP//AAD4HwAA7/cAAN/7AAD//wAAoYUAAJ55AACf+QAAh+EAAAAAAADAAwAA4AcAAP5/AAD//wAA//8AAA=="/>
<link rel="icon" type="image/png" sizes="32x32" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH4QQQEwksSS9ZWwAAAk1JREFUWMPtll2ITVEUx39nn/O7Y5qR8f05wtCUUr6ZIS++8pEnkZInPImneaCQ5METNdOkeFBKUhMPRIkHKfEuUZSUlGlKPN2TrgfncpvmnntnmlEyq1Z7t89/rf9a6+y99oZxGZf/XeIq61EdtgKXgdXA0xrYAvBjOIF1AI9zvjcC74BSpndrJPkBWDScTF8Aa4E3wDlgHbASaANmVqlcCnwHvgDvgVfAJ+AikAAvgfVZwLnSVZHZaOuKoQi3ZOMi4NkYkpe1p4J7A8BpYAD49hfIy/oqG0+hLomiKP2L5L+1ubn5115S+3OAn4EnwBlgMzCjyt6ZAnQCJ4A7wOs88iRJHvw50HoujuPBoCKwHWiosy8MdfZnAdcHk8dxXFJ3VQbQlCTJvRBCGdRbD4M6uc5glpY3eAihpN5S5w12diSEcCCEcKUO4ljdr15T76ur1FDDLIQQ3qv71EdDOe3Kxj3leRXyk+pxdWnFWod6Wt2bY3de3aSuUHcPBVimHs7mK9WrmeOF6lR1o9qnzskh2ar2qm1qizpfXaPeVGdlmGN5pb09qMxz1Xb1kLqgzn1RyH7JUXW52lr5e/Kqi9qpto7V1atuUzfnARrV7jEib1T76gG2qxdGmXyiekkt1GswPTtek0aBfJp6YySGBfWg2tPQ0FAYgf1stUfdmdcjarbYJEniKIq6gY/Aw+zWHAC+p2labGpqiorFYgGYCEzN7oQdQClN07O1/EfDyGgC0ALMBdYAi4FyK+4H3gLPsxfR1zRNi+NP7nH5J+QntnXe5B5mpfQAAAAASUVORK5CYII=">

<!-- Google fonts -->
<link href='https://fonts.googleapis.com/css?family=Lato:400,600,900' rel='stylesheet' type='text/css'/>
<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<!-- CSS -->
<title>shapeshifting</title>
<link rel="stylesheet" type="text/css" href="nimdoc.out.css">

<script type="text/javascript" src="dochack.js"></script>

<script type="text/javascript">
function main() {
  var pragmaDots = document.getElementsByClassName("pragmadots");
  for (var i = 0; i < pragmaDots.length; i++) {
    pragmaDots[i].onclick = function(event) {
      // Hide tease
      event.target.parentNode.style.display = "none";
      // Show actual
      event.target.parentNode.nextElementSibling.style.display = "inline";
    }
  }

  const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
  function switchTheme(e) {
      if (e.target.checked) {
          document.documentElement.setAttribute('data-theme', 'dark');
          localStorage.setItem('theme', 'dark');
      } else {
          document.documentElement.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
      }
  }

  toggleSwitch.addEventListener('change', switchTheme, false);


  if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
    document.documentElement.setAttribute('data-theme', "dark");
    toggleSwitch.checked = true;
  } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: light)').matches) {
    document.documentElement.setAttribute('data-theme', "light");
    toggleSwitch.checked = false;
  } else {
    const currentTheme = localStorage.getItem('theme') ? localStorage.getItem('theme') : null;
    if (currentTheme) {
      document.documentElement.setAttribute('data-theme', currentTheme);

      if (currentTheme === 'dark') {
        toggleSwitch.checked = true;
      }
    }
  }
}
</script>

</head>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Arraymancer - shapeshifting</title>

<link href="docutils.css" rel="stylesheet" type="text/css"/>
<link href="nav.css" rel="stylesheet" type="text/css"/>

<link href='http://fonts.googleapis.com/css?family=Raleway:400,600,900' rel='stylesheet' type='text/css'/>
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<a href="https://github.com/mratsim/arraymancer"><img style="position: fixed; top: 0; right: 0; border: 0; z-index: 10;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>

<body onload="main()">
<div class="document" id="documentId">
  <div class="container">
    <h1 class="title">shapeshifting</h1>
    <div class="row">
  <div class="three columns">
  <div class="theme-switch-wrapper">
    <label class="theme-switch" for="checkbox">
      <input type="checkbox" id="checkbox" />
      <div class="slider round"></div>
    </label>
    &nbsp;&nbsp;&nbsp; <em>Dark Mode</em>
  </div>
  <div id="global-links">
    <ul class="simple">
    </ul>
  </div>
  <div id="searchInputDiv">
    Search: <input type="text" id="searchInput"
      onkeyup="search()" />
  </div>
  <div>
    Group by:
    <select onchange="groupBy(this.value)">
      <option value="section">Section</option>
      <option value="type">Type</option>
    </select>
  </div>
  <ul class="simple simple-toc" id="toc-list">
<li>
  <a class="reference reference-toplevel" href="#6" id="56">Imports</a>
  <ul class="simple simple-toc-section">
    
  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#12" id="62">Procs</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#transpose%2CTensor"
    title="transpose(t: Tensor): Tensor"><wbr />transpose<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#asContiguous%2CTensor%5BT%5D%2COrderType%2Cbool"
    title="asContiguous[T](t: Tensor[T]; layout: OrderType = rowMajor; force: bool = false): Tensor[T]"><wbr />as<wbr />Contiguous<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#reshape%2CTensor%2Cvarargs%5Bint%5D"
    title="reshape(t: Tensor; new_shape: varargs[int]): Tensor"><wbr />reshape<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#reshape%2CTensor%2CMetadataArray"
    title="reshape(t: Tensor; new_shape: MetadataArray): Tensor"><wbr />reshape<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#broadcast%2CTensor%5BT%5D%2Cvarargs%5Bint%5D"
    title="broadcast[T](t: Tensor[T]; shape: varargs[int]): Tensor[T]"><wbr />broadcast<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#broadcast%2CTensor%5BT%5D%2CMetadataArray"
    title="broadcast[T](t: Tensor[T]; shape: MetadataArray): Tensor[T]"><wbr />broadcast<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#broadcast%2CT%2Cvarargs%5Bint%5D"
    title="broadcast[T: SomeNumber](val: T; shape: varargs[int]): Tensor[T]"><wbr />broadcast<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#broadcast%2CT%2CMetadataArray"
    title="broadcast[T: SomeNumber](val: T; shape: MetadataArray): Tensor[T]"><wbr />broadcast<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#broadcast2%2CTensor%5BT%5D%2CTensor%5BT%5D"
    title="broadcast2[T](a, b: Tensor[T]): tuple[a, b: Tensor[T]]"><wbr />broadcast2<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#permute%2CTensor%2Cvarargs%5Bint%5D"
    title="permute(t: Tensor; dims: varargs[int]): Tensor"><wbr />permute<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#concat%2Cvarargs%5BTensor%5BT%5D%5D%2Cint"
    title="concat[T](t_list: varargs[Tensor[T]]; axis: int): Tensor[T]"><wbr />concat<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#stack%2Cvarargs%5BTensor%5BT%5D%5D%2CNatural"
    title="stack[T](tensors: varargs[Tensor[T]]; axis: Natural = 0): Tensor[T]"><wbr />stack<span class="attachedType"></span></a></li>

  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#13" id="63">Funcs</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#squeeze%2CAnyTensor"
    title="squeeze(t: AnyTensor): AnyTensor"><wbr />squeeze<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#squeeze%2CTensor%2CNatural"
    title="squeeze(t: Tensor; axis: Natural): Tensor"><wbr />squeeze<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#unsqueeze%2CTensor%2CNatural"
    title="unsqueeze(t: Tensor; axis: Natural): Tensor"><wbr />unsqueeze<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#split%2CTensor%5BT%5D%2CPositive%2CNatural"
    title="split[T](t: Tensor[T]; chunk_size: Positive; axis: Natural): seq[Tensor[T]]"><wbr />split<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#chunk%2CTensor%5BT%5D%2CPositive%2CNatural"
    title="chunk[T](t: Tensor[T]; nb_chunks: Positive; axis: Natural): seq[Tensor[T]]"><wbr />chunk<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#index_select%2CTensor%5BT%5D%2Cint%2CTensor%5BIdx%3A+byte+or+char+or+int+or+int8+or+int16+or+int32+or+int64+or+uint+or+uint8+or+uint16+or+uint32+or+uint64+or+float+or+float32+or+float64%5D"
    title="index_select[T; Idx: byte or char or SomeNumber](t: Tensor[T]; axis: int;
    indices: Tensor[Idx]): Tensor[T]"><wbr />index_<wbr />select<span class="attachedType"></span></a></li>

  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#18" id="68">Templates</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#bc.t%2C%2Cvarargs%5Bint%5D"
    title="bc(t: (Tensor | SomeNumber); shape: varargs[int]): untyped"><wbr />bc<span class="attachedType"></span></a></li>
  <li><a class="reference" href="#bc.t%2C%2CMetadataArray"
    title="bc(t: (Tensor | SomeNumber); shape: MetadataArray): untyped"><wbr />bc<span class="attachedType"></span></a></li>

  </ul>
</li>

</ul>

  </div>
  <div class="nine columns" id="content">
  <div id="tocRoot"></div>
  
  <p class="module-desc"></p>
  <div class="section" id="6">
<h1><a class="toc-backref" href="#6">Imports</a></h1>
<dl class="item">
<a class="reference external" href="metadataArray.html">metadataArray</a>, <a class="reference external" href="p_shapeshifting.html">p_shapeshifting</a>, <a class="reference external" href="p_checks.html">p_checks</a>, <a class="reference external" href="p_accessors_macros_write.html">p_accessors_macros_write</a>, <a class="reference external" href="accessors.html">accessors</a>, <a class="reference external" href="data_structure.html">data_structure</a>, <a class="reference external" href="init_cpu.html">init_cpu</a>, <a class="reference external" href="higher_order_applymap.html">higher_order_applymap</a>
</dl></div>
<div class="section" id="12">
<h1><a class="toc-backref" href="#12">Procs</a></h1>
<dl class="item">
<a id="transpose,Tensor"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#transpose%2CTensor"><span class="Identifier">transpose</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span><span class="Other">,</span> <span class="Identifier">noSideEffect</span><span class="Other">,</span> <span class="Identifier">inline</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

<p>Transpose a Tensor.</p>
<p>For N-d Tensor with shape (0, 1, 2 ... n-1) the resulting tensor will have shape (n-1, ... 2, 1, 0)</p>
<p>Data is not copied or modified, only metadata is modified.</p>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L27"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L27" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="asContiguous,Tensor[T],OrderType,bool"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#asContiguous%2CTensor%5BT%5D%2COrderType%2Cbool"><span class="Identifier">asContiguous</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">layout</span><span class="Other">:</span> <span class="Identifier">OrderType</span> <span class="Other">=</span> <span class="Identifier">rowMajor</span><span class="Other">;</span> <span class="Identifier">force</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span>
    <span class="Identifier">T</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

<p>Transform a tensor with general striding to a Tensor with contiguous layout.</p>
<p>By default tensor will be rowMajor.</p>
<p>The layout is kept if the tensor is already contiguous (C Major or F major) The &quot;force&quot; parameter can force re-ordering to a specific layout.</p>
<p>Result is always a fully packed tensor even if the input is a contiguous slice.</p>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L38"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L38" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="reshape,Tensor,varargs[int]"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#reshape%2CTensor%2Cvarargs%5Bint%5D"><span class="Identifier">reshape</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">new_shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Reshape a tensor. If possible no data copy is done and the returned tensor shares data with the input. If input is not contiguous, this is not possible and a copy will be made.<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a tensor</li>
<li>a new shape. Number of elements must be the same</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with the same data but reshaped.</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L59"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L59" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="reshape,Tensor,MetadataArray"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#reshape%2CTensor%2CMetadataArray"><span class="Identifier">reshape</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">new_shape</span><span class="Other">:</span> <a href="metadataArray.html#MetadataArray"><span class="Identifier">MetadataArray</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Reshape a tensor. If possible no data copy is done and the returned tensor shares data with the input. If input is not contiguous, this is not possible and a copy will be made.<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a tensor</li>
<li>a new shape. Number of elements must be the same</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with the same data but reshaped.</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L71"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L71" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="broadcast,Tensor[T],varargs[int]"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CTensor%5BT%5D%2Cvarargs%5Bint%5D"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span><span class="Other">,</span> <span class="Identifier">noSideEffect</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

<p>Explicitly broadcast a tensor to the specified shape.</p>
<p>Dimension(s) of size 1 can be expanded to arbitrary size by replicating values along that dimension.</p>
<dl class="docutils"><dt>Warning ⚠:</dt>
<dd>A broadcasted tensor should not be modified and only used for computation.</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L83"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L83" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="broadcast,Tensor[T],MetadataArray"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CTensor%5BT%5D%2CMetadataArray"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <a href="metadataArray.html#MetadataArray"><span class="Identifier">MetadataArray</span></a><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span><span class="Other">,</span> <span class="Identifier">noSideEffect</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

<p>Explicitly broadcast a tensor to the specified shape.</p>
<p>Dimension(s) of size 1 can be expanded to arbitrary size by replicating values along that dimension.</p>
<dl class="docutils"><dt>Warning ⚠:</dt>
<dd>A broadcasted tensor should not be modified and only used for computation.</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L95"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L95" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="broadcast,T,varargs[int]"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CT%2Cvarargs%5Bint%5D"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">:</span> <span class="Identifier">SomeNumber</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">val</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span><span class="Other">,</span>
    <span class="Identifier">noSideEffect</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Broadcast a number<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a number to be broadcasted</li>
<li>a tensor shape that will be broadcasted to</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with the broadcasted shape where all elements has the broadcasted value</li>
</ul>
</dd>
</dl>
<p>The broadcasting is made using tensor data of size 1 and 0 strides, i.e. the operation is memory efficient.</p>
<dl class="docutils"><dt>Warning ⚠:</dt>
<dd>A broadcasted tensor should not be modified and only used for computation. Modifying any value from this broadcasted tensor will change all its values.</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L107"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L107" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="broadcast,T,MetadataArray"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#broadcast%2CT%2CMetadataArray"><span class="Identifier">broadcast</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">:</span> <span class="Identifier">SomeNumber</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">val</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <a href="metadataArray.html#MetadataArray"><span class="Identifier">MetadataArray</span></a><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span><span class="Other">,</span>
    <span class="Identifier">noSideEffect</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Broadcast a number<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a number to be broadcasted</li>
<li>a tensor shape that will be broadcasted to</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with the broadcasted shape where all elements has the broadcasted value</li>
</ul>
</dd>
</dl>
<p>The broadcasting is made using tensor data of size 1 and 0 strides, i.e. the operation is memory efficient.</p>
<dl class="docutils"><dt>Warning ⚠:</dt>
<dd>A broadcasted tensor should not be modified and only used for computation. Modifying any value from this broadcasted tensor will change all its values.</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L127"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L127" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="broadcast2,Tensor[T],Tensor[T]"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#broadcast2%2CTensor%5BT%5D%2CTensor%5BT%5D"><span class="Identifier">broadcast2</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">a</span><span class="Other">,</span> <span class="Identifier">b</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Keyword">tuple</span><span class="Other">[</span><span class="Identifier">a</span><span class="Other">,</span> <span class="Identifier">b</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noSideEffect</span><span class="Other">,</span> <span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

<p>Broadcast 2 tensors so they have compatible shapes for element-wise computations.</p>
<p>Tensors in the tuple can be accessed with output.a and output.b</p>
<p>The returned broadcasted Tensors share the underlying data with the input.</p>
<p>Dimension(s) of size 1 can be expanded to arbitrary size by replicating values along that dimension.</p>
<dl class="docutils"><dt>Warning ⚠:</dt>
<dd>This is a no-copy operation, data is shared with the input. This proc does not guarantee that a <tt class="docutils literal"><span class="pre">let</span></tt> value is immutable. A broadcasted tensor should not be modified and only used for computation.</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L155"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L155" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="permute,Tensor,varargs[int]"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#permute%2CTensor%2Cvarargs%5Bint%5D"><span class="Identifier">permute</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">dims</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span><span class="Other">,</span> <span class="Identifier">noSideEffect</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Permute dimensions of a tensors<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a tensor</li>
<li>the new dimension order</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with re-order dimension</li>
</ul>
</dd>
<dt>Usage:</dt>
<dd><pre class="listing"><span class="Identifier">a</span><span class="Operator">.</span><span class="Identifier">permute</span><span class="Punctuation">(</span><span class="DecNumber">0</span><span class="Punctuation">,</span><span class="DecNumber">2</span><span class="Punctuation">,</span><span class="DecNumber">1</span><span class="Punctuation">)</span> <span class="Comment"># dim 0 stays at 0, dim 1 becomes dim 2 and dim 2 becomes dim 1</span></pre></dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L176"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L176" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="concat,varargs[Tensor[T]],int"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#concat%2Cvarargs%5BTensor%5BT%5D%5D%2Cint"><span class="Identifier">concat</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t_list</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">int</span><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Concatenate tensors<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>Tensors</li>
<li>An axis (dimension)</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L191"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L191" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="stack,varargs[Tensor[T]],Natural"></a>
<dt><pre><span class="Keyword">proc</span> <a href="#stack%2Cvarargs%5BTensor%5BT%5D%5D%2CNatural"><span class="Identifier">stack</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">tensors</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span> <span class="Other">=</span> <span class="DecNumber">0</span><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Join a sequence of tensors along a new axis into a new tensor.<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a tensor</li>
<li>an axis (dimension)</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a new stacked tensor along the new axis</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L256"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L256" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>
<div class="section" id="13">
<h1><a class="toc-backref" href="#13">Funcs</a></h1>
<dl class="item">
<a id="squeeze,AnyTensor"></a>
<dt><pre><span class="Keyword">func</span> <a href="#squeeze%2CAnyTensor"><span class="Identifier">squeeze</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#AnyTensor"><span class="Identifier">AnyTensor</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">AnyTensor</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Squeeze tensors. For example a Tensor of shape [4,1,3] will become [4,3]<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a tensor</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with singleton dimensions collapsed</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L226"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L226" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="squeeze,Tensor,Natural"></a>
<dt><pre><span class="Keyword">func</span> <a href="#squeeze%2CTensor%2CNatural"><span class="Identifier">squeeze</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Collapse the given axis, if the dimension is not 1, it does nothing.<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a tensor</li>
<li>an axis (dimension)</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with that axis collapsed, if it was a singleton dimension</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L235"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L235" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="unsqueeze,Tensor,Natural"></a>
<dt><pre><span class="Keyword">func</span> <a href="#unsqueeze%2CTensor%2CNatural"><span class="Identifier">unsqueeze</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Insert a new axis just before the given axis, increasing the tensor dimension (rank) by 1<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>a tensor</li>
<li>an axis (dimension)</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>a tensor with that new axis</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L245"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L245" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="split,Tensor[T],Positive,Natural"></a>
<dt><pre><span class="Keyword">func</span> <a href="#split%2CTensor%5BT%5D%2CPositive%2CNatural"><span class="Identifier">split</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">chunk_size</span><span class="Other">:</span> <span class="Identifier">Positive</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma">
    <span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

Split the tensor into chunks of size <tt class="docutils literal"><span class="pre">chunk_size</span></tt> along the specified axis. Last chunk size will equal the remainder if the specified axis length is not divisible by <tt class="docutils literal"><span class="pre">chunk_size</span></tt>
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L266"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L266" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="chunk,Tensor[T],Positive,Natural"></a>
<dt><pre><span class="Keyword">func</span> <a href="#chunk%2CTensor%5BT%5D%2CPositive%2CNatural"><span class="Identifier">chunk</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">nb_chunks</span><span class="Other">:</span> <span class="Identifier">Positive</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">Natural</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">seq</span><span class="Other">[</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">]</span> <span><span class="Other">{</span><span class="Other pragmadots">...</span><span class="Other">}</span></span><span class="pragmawrap"><span class="Other">{.</span><span class="pragma"><span class="Identifier">noInit</span></span><span class="Other">.}</span></span></pre></dt>
<dd>

<p>Splits a Tensor into n chunks along the specified axis.</p>
<p>In case a tensor cannot be split evenly, with la == length_axis, n = n_chunks</p>
<dl class="docutils"><dt>it returns la mod n subtensors of size <tt class="docutils literal"><span class="pre">(la div n) + 1</span></tt></dt>
<dd>the rest of size <tt class="docutils literal"><span class="pre">la div n</span></tt>.</dd>
</dl>
<p>This is consistent with numpy array_split</p>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L282"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L282" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="index_select,Tensor[T],int,Tensor[Idx: byte or char or int or int8 or int16 or int32 or int64 or uint or uint8 or uint16 or uint32 or uint64 or float or float32 or float64]"></a>
<dt><pre><span class="Keyword">func</span> <a href="#index_select%2CTensor%5BT%5D%2Cint%2CTensor%5BIdx%3A+byte+or+char+or+int+or+int8+or+int16+or+int32+or+int64+or+uint+or+uint8+or+uint16+or+uint32+or+uint64+or+float+or+float32+or+float64%5D"><span class="Identifier">index_select</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">Idx</span><span class="Other">:</span> <span class="Identifier">byte</span> <span class="Keyword">or</span> <span class="Identifier">char</span> <span class="Keyword">or</span> <span class="Identifier">SomeNumber</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">axis</span><span class="Other">:</span> <span class="Identifier">int</span><span class="Other">;</span>
    <span class="Identifier">indices</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">Idx</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
<dd>

Take elements from a tensor along an axis using the indices Tensor. This is equivalent to NumPy <tt class="docutils literal"><span class="pre">take</span></tt>. The result does not share the input storage, there are copies. The tensors containing the indices can be an integer, byte or char tensor.
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L303"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L303" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>
<div class="section" id="18">
<h1><a class="toc-backref" href="#18">Templates</a></h1>
<dl class="item">
<a id="bc.t,,varargs[int]"></a>
<dt><pre><span class="Keyword">template</span> <a href="#bc.t%2C%2Cvarargs%5Bint%5D"><span class="Identifier">bc</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <span class="Other">(</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a> <span class="Operator">|</span> <span class="Identifier">SomeNumber</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <span class="Identifier">varargs</span><span class="Other">[</span><span class="Identifier">int</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">untyped</span></pre></dt>
<dd>

Alias for <tt class="docutils literal"><span class="pre">broadcast</span></tt>
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L147"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L147" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<a id="bc.t,,MetadataArray"></a>
<dt><pre><span class="Keyword">template</span> <a href="#bc.t%2C%2CMetadataArray"><span class="Identifier">bc</span></a><span class="Other">(</span><span class="Identifier">t</span><span class="Other">:</span> <span class="Other">(</span><a href="data_structure.html#Tensor"><span class="Identifier">Tensor</span></a> <span class="Operator">|</span> <span class="Identifier">SomeNumber</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">shape</span><span class="Other">:</span> <a href="metadataArray.html#MetadataArray"><span class="Identifier">MetadataArray</span></a><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">untyped</span></pre></dt>
<dd>

Alias for <tt class="docutils literal"><span class="pre">broadcast</span></tt>
&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/tensor/shapeshifting.nim#L151"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/tensor/shapeshifting.nim#L151" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>

  </div>
</div>

    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small style="color: var(--hint);">Made with Nim. Generated: 2020-04-06 09:47:14 UTC</small>
      </div>
    </div>
  </div>
</div>

<header>
  <a class="pagetitle" href="index.html">Arraymancer</a>
  <span>
    <a href="#">Technical reference</a>
    <ul class="monospace">
      <span>
        <a href="#">Core tensor API</a>
        <ul class="monospace">
          <li><a href="accessors.html">accessors</a></li>
<li><a href="accessors_macros_read.html">accessors_macros_read</a></li>
<li><a href="accessors_macros_syntax.html">accessors_macros_syntax</a></li>
<li><a href="accessors_macros_write.html">accessors_macros_write</a></li>
<li><a href="aggregate.html">aggregate</a></li>
<li><a href="blas_l3_gemm.html">blas_l3_gemm</a></li>
<li><a href="cublas.html">cublas</a></li>
<li><a href="cuda.html">cuda</a></li>
<li><a href="cuda_global_state.html">cuda_global_state</a></li>
<li><a href="data_structure.html">data_structure</a></li>
<li><a href="display.html">display</a></li>
<li><a href="display_cuda.html">display_cuda</a></li>
<li><a href="einsum.html">einsum</a></li>
<li><a href="exporting.html">exporting</a></li>
<li><a href="filling_data.html">filling_data</a></li>
<li><a href="higher_order_applymap.html">higher_order_applymap</a></li>
<li><a href="higher_order_foldreduce.html">higher_order_foldreduce</a></li>
<li><a href="incl_accessors_cuda.html">incl_accessors_cuda</a></li>
<li><a href="incl_higher_order_cuda.html">incl_higher_order_cuda</a></li>
<li><a href="incl_kernels_cuda.html">incl_kernels_cuda</a></li>
<li><a href="init_copy_cpu.html">init_copy_cpu</a></li>
<li><a href="init_copy_cuda.html">init_copy_cuda</a></li>
<li><a href="init_cpu.html">init_cpu</a></li>
<li><a href="init_cuda.html">init_cuda</a></li>
<li><a href="init_opencl.html">init_opencl</a></li>
<li><a href="lapack.html">lapack</a></li>
<li><a href="math_functions.html">math_functions</a></li>
<li><a href="memory_optimization_hints.html">memory_optimization_hints</a></li>
<li><a href="metadataArray.html">metadataArray</a></li>
<li><a href="naive_l2_gemv.html">naive_l2_gemv</a></li>
<li><a href="opencl_backend.html">opencl_backend</a></li>
<li><a href="opencl_global_state.html">opencl_global_state</a></li>
<li><a href="openmp.html">openmp</a></li>
<li><a href="operators_blas_l1.html">operators_blas_l1</a></li>
<li><a href="operators_blas_l1_cuda.html">operators_blas_l1_cuda</a></li>
<li><a href="operators_blas_l1_opencl.html">operators_blas_l1_opencl</a></li>
<li><a href="operators_blas_l2l3.html">operators_blas_l2l3</a></li>
<li><a href="operators_blas_l2l3_cuda.html">operators_blas_l2l3_cuda</a></li>
<li><a href="operators_blas_l2l3_opencl.html">operators_blas_l2l3_opencl</a></li>
<li><a href="operators_broadcasted.html">operators_broadcasted</a></li>
<li><a href="operators_broadcasted_cuda.html">operators_broadcasted_cuda</a></li>
<li><a href="operators_broadcasted_opencl.html">operators_broadcasted_opencl</a></li>
<li><a href="operators_comparison.html">operators_comparison</a></li>
<li><a href="operators_logical.html">operators_logical</a></li>
<li><a href="optim_ops_fusion.html">optim_ops_fusion</a></li>
<li><a href="p_accessors.html">p_accessors</a></li>
<li><a href="p_accessors_macros_desugar.html">p_accessors_macros_desugar</a></li>
<li><a href="p_accessors_macros_read.html">p_accessors_macros_read</a></li>
<li><a href="p_accessors_macros_write.html">p_accessors_macros_write</a></li>
<li><a href="p_checks.html">p_checks</a></li>
<li><a href="p_complex.html">p_complex</a></li>
<li><a href="p_display.html">p_display</a></li>
<li><a href="p_init_cpu.html">p_init_cpu</a></li>
<li><a href="p_init_cuda.html">p_init_cuda</a></li>
<li><a href="p_init_opencl.html">p_init_opencl</a></li>
<li><a href="p_kernels_interface_cuda.html">p_kernels_interface_cuda</a></li>
<li><a href="p_kernels_interface_opencl.html">p_kernels_interface_opencl</a></li>
<li><a href="p_operator_blas_l2l3.html">p_operator_blas_l2l3</a></li>
<li><a href="p_shapeshifting.html">p_shapeshifting</a></li>
<li><a href="shapeshifting.html">shapeshifting</a></li>
<li><a href="shapeshifting_cuda.html">shapeshifting_cuda</a></li>
<li><a href="shapeshifting_opencl.html">shapeshifting_opencl</a></li>
<li><a href="syntactic_sugar.html">syntactic_sugar</a></li>
<li><a href="tensor.html">tensor</a></li>
<li><a href="tensor_cuda.html">tensor_cuda</a></li>
<li><a href="tensor_opencl.html">tensor_opencl</a></li>
<li><a href="ufunc.html">ufunc</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neural network API</a>
        <ul class="monospace">
          <li><a href="conv2D.html">Layers: Convolution 2D</a></li>
<li><a href="cross_entropy_losses.html">Loss: Cross-Entropy losses</a></li>
<li><a href="dsl_core.html">Neural network: Declaration</a></li>
<li><a href="dsl_forwardsugar.html">dsl_forwardsugar</a></li>
<li><a href="dsl_initialization.html">dsl_initialization</a></li>
<li><a href="dsl_topology.html">dsl_topology</a></li>
<li><a href="dsl_types.html">dsl_types</a></li>
<li><a href="dsl_utils.html">dsl_utils</a></li>
<li><a href="embedding.html">Layers: Embedding</a></li>
<li><a href="gru.html">Layers: GRU (Gated Linear Unit)</a></li>
<li><a href="init.html">Layers: Initializations</a></li>
<li><a href="linear.html">Layers: Linear/Dense</a></li>
<li><a href="maxpool2D.html">Layers: Maxpool 2D</a></li>
<li><a href="mean_square_error_loss.html">Loss: Mean Square Error</a></li>
<li><a href="optimizers.html">Optimizers</a></li>
<li><a href="relu.html">Activation: Relu (Rectified linear Unit)</a></li>
<li><a href="sigmoid.html">Activation: Sigmoid</a></li>
<li><a href="softmax.html">Softmax</a></li>
<li><a href="tanh.html">Activation: Tanh</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Linear algebra, stats, ML</a>
        <ul class="monospace">
          <li><a href="accuracy_score.html">Accuracy score</a></li>
<li><a href="auxiliary_blas.html">auxiliary_blas</a></li>
<li><a href="auxiliary_lapack.html">auxiliary_lapack</a></li>
<li><a href="common_error_functions.html">Common errors, MAE and MSE (L1, L2 loss)</a></li>
<li><a href="decomposition.html">Eigenvalue decomposition</a></li>
<li><a href="decomposition_lapack.html">decomposition_lapack</a></li>
<li><a href="decomposition_rand.html">Randomized Truncated SVD</a></li>
<li><a href="init_colmajor.html">init_colmajor</a></li>
<li><a href="kmeans.html">K-Means</a></li>
<li><a href="least_squares.html">Least squares solver</a></li>
<li><a href="least_squares_lapack.html">least_squares_lapack</a></li>
<li><a href="linear_algebra.html">linear_algebra</a></li>
<li><a href="linear_systems.html">Linear systems solver</a></li>
<li><a href="overload.html">overload</a></li>
<li><a href="pca.html">Principal Component Analysis (PCA)</a></li>
<li><a href="solve_lapack.html">solve_lapack</a></li>
<li><a href="special_matrices.html">Special linear algebra matrices</a></li>
<li><a href="stats.html">Statistics</a></li>
<li><a href="triangular.html">triangular</a></li>
        </ul>
      </span>
      <span>
        <a href="#">IO & Datasets</a>
        <ul class="monospace">
          <li><a href="imdb.html">IMDB</a></li>
<li><a href="io_csv.html">CSV reading and writing</a></li>
<li><a href="io_hdf5.html">HDF5 files reading and writing</a></li>
<li><a href="io_image.html">Images reading and writing</a></li>
<li><a href="io_npy.html">Numpy files reading and writing</a></li>
<li><a href="io_stream_readers.html">io_stream_readers</a></li>
<li><a href="mnist.html">MNIST</a></li>
<li><a href="util.html">util</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Autograd</a>
        <ul class="monospace">
          <li><a href="autograd_common.html">Data structure</a></li>
<li><a href="gates_basic.html">Basic operations</a></li>
<li><a href="gates_blas.html">Linear algebra operations</a></li>
<li><a href="gates_hadamard.html">Hadamard product (elementwise matrix multiply)</a></li>
<li><a href="gates_reduce.html">Reduction operations</a></li>
<li><a href="gates_shapeshifting_concat_split.html">Concatenation, stacking, splitting, chunking operations</a></li>
<li><a href="gates_shapeshifting_views.html">Linear algebra operations</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neuralnet primitives</a>
        <ul class="monospace">
          <li><a href="conv.html">conv</a></li>
<li><a href="cudnn.html">cudnn</a></li>
<li><a href="cudnn_conv_interface.html">cudnn_conv_interface</a></li>
<li><a href="nn_primitives.html">nn_primitives</a></li>
<li><a href="nnp_activation.html">Activations</a></li>
<li><a href="nnp_conv2d_cudnn.html">Convolution 2D - CuDNN</a></li>
<li><a href="nnp_convolution.html">Convolution 2D</a></li>
<li><a href="nnp_embedding.html">Embeddings</a></li>
<li><a href="nnp_gru.html">Gated Recurrent Unit (GRU)</a></li>
<li><a href="nnp_linear.html">Linear / Dense layer</a></li>
<li><a href="nnp_maxpooling.html">Maxpooling</a></li>
<li><a href="nnp_numerical_gradient.html">Numerical gradient</a></li>
<li><a href="nnp_sigmoid_cross_entropy.html">Sigmoid Cross-Entropy loss</a></li>
<li><a href="nnp_softmax.html">Softmax</a></li>
<li><a href="nnp_softmax_cross_entropy.html">Softmax Cross-Entropy loss</a></li>
<li><a href="nnpack.html">nnpack</a></li>
<li><a href="nnpack_interface.html">nnpack_interface</a></li>
<li><a href="p_activation.html">p_activation</a></li>
<li><a href="p_logsumexp.html">p_logsumexp</a></li>
<li><a href="p_nnp_checks.html">p_nnp_checks</a></li>
<li><a href="p_nnp_types.html">p_nnp_types</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Other docs</a>
        <ul class="monospace">
          <li><a href="align_unroller.html">align_unroller</a></li>
<li><a href="ast_utils.html">ast_utils</a></li>
<li><a href="compiler_optim_hints.html">compiler_optim_hints</a></li>
<li><a href="cpuinfo_x86.html">cpuinfo_x86</a></li>
<li><a href="functional.html">functional</a></li>
<li><a href="gemm.html">gemm</a></li>
<li><a href="gemm_packing.html">gemm_packing</a></li>
<li><a href="gemm_prepacked.html">gemm_prepacked</a></li>
<li><a href="gemm_tiling.html">gemm_tiling</a></li>
<li><a href="gemm_ukernel_avx.html">gemm_ukernel_avx</a></li>
<li><a href="gemm_ukernel_avx2.html">gemm_ukernel_avx2</a></li>
<li><a href="gemm_ukernel_avx512.html">gemm_ukernel_avx512</a></li>
<li><a href="gemm_ukernel_avx_fma.html">gemm_ukernel_avx_fma</a></li>
<li><a href="gemm_ukernel_dispatch.html">gemm_ukernel_dispatch</a></li>
<li><a href="gemm_ukernel_generator.html">gemm_ukernel_generator</a></li>
<li><a href="gemm_ukernel_generic.html">gemm_ukernel_generic</a></li>
<li><a href="gemm_ukernel_sse.html">gemm_ukernel_sse</a></li>
<li><a href="gemm_ukernel_sse2.html">gemm_ukernel_sse2</a></li>
<li><a href="gemm_ukernel_sse4_1.html">gemm_ukernel_sse4_1</a></li>
<li><a href="gemm_utils.html">gemm_utils</a></li>
<li><a href="global_config.html">global_config</a></li>
<li><a href="math_ops_fusion.html">math_ops_fusion</a></li>
<li><a href="memory.html">memory</a></li>
<li><a href="nested_containers.html">nested_containers</a></li>
<li><a href="nlp.html">nlp</a></li>
<li><a href="openmp.html">openmp</a></li>
<li><a href="sequninit.html">sequninit</a></li>
<li><a href="simd.html">simd</a></li>
<li><a href="tokenizers.html">tokenizers</a></li>
        </ul>
      </span>
    </ul>
  </span>
  <span>
    <a href="#">Tutorial</a>
    <ul class="monospace">
      <li><a href="tuto.first_steps.html">First steps</a></li>
      <li><a href="tuto.slicing.html">Taking a slice of a tensor</a></li>
      <li><a href="tuto.linear_algebra.html">Matrix & vectors operations</a></li>
      <li><a href="tuto.broadcasting.html">Broadcasted operations</a></li>
      <li><a href="tuto.shapeshifting.html">Transposing, Reshaping, Permuting, Concatenating</a></li>
      <li><a href="tuto.map_reduce.html">Map & Reduce</a></li>
      <li><a href="tuto.iterators.html">Basic iterators</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Spellbook (How-To&apos;s)</a>
    <ul class="monospace">
      <li><a href="howto.type_conversion.html">How to convert a Tensor type?</a></li>
      <li><a href="howto.ufunc.html">How to create a new universal function?</a></li>
      <li><a href="howto.perceptron.html">How to create a multilayer perceptron?</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Under the hood</a>
    <ul class="monospace">
      <li><a href="uth.speed.html">How Arraymancer achieves its speed?</a></li>
      <li><a href="uth.copy_semantics.html">Why does `=` share data by default aka reference semantics?</a></li>
      <li><a href="uth.opencl_cuda_nim.html">Working with OpenCL and Cuda in Nim</a></li>
    </ul>
  </span>
</header>
</body>
</html>
